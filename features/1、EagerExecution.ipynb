{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TensorFlow 的 Eager Execution 是一种命令式编程环境，可立即评估操作，无需构建图：操作会返回具体的值，而不是构建以后再运行的计算图。这样能让您轻松地开始使用 TensorFlow 和调试模型，并且还减少了样板代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,[[4.]]\n"
     ]
    }
   ],
   "source": [
    "# 此句主要是让python2.7的代码也能在python3上面运行\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 开启Eager Execution\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "x = [[2.0]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello,{}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.add(a, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n[[ 2  6]\n [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n [12 20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Tensor.numpy 方法返回对象的值作为 NumPy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "动态控制流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FizeeBuzee\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\nFizee\ntf.Tensor(4, shape=(), dtype=int32)\nBuzz\nFizee\ntf.Tensor(7, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\nFizee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=98, shape=(), dtype=int32, numpy=10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    for num in range(max_num):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print(\"FizeeBuzee\")\n",
    "        elif int(num % 3) == 0:\n",
    "            print(\"Fizee\")\n",
    "        elif int(num % 5) == 0:\n",
    "            print(\"Buzz\")\n",
    "        else:\n",
    "            print(num)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "fizzbuzz(10)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.Dense 表示全连接层\n",
    "#tf.keras.qusuential 表示由现行堆叠的模型\n",
    "# 以下两种生成模型的方法效果相同\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n",
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.Dense1 = tf.keras.layers.Dense(units=10)\n",
    "        self.Dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, input):\n",
    "        result = self.Dense1(input)\n",
    "        result = self.Dense2(input)\n",
    "        result = self.Dense2(input)\n",
    "        return result\n",
    "\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=181, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "w = tfe.Variable([[1.0]])\n",
    "# tf.GradientTape()跟踪操作，可以计算梯度\n",
    "with tf.GradientTape()as tape:\n",
    "    loss = w * w\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "简单训练模型的一个完整实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 67.669\nLoss as step :000:65.079\nLoss as step :020:30.041\nLoss as step :040:14.190\nLoss as step :060:7.008\nLoss as step :080:3.750\nLoss as step :100:2.270\nLoss as step :120:1.597\nLoss as step :140:1.290\nLoss as step :160:1.150\nLoss as step :180:1.086\nW = 3.0622386932373047,B = 2.146221399307251\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "def loss(weights, biases):\n",
    "    error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(weights, biases)\n",
    "    return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W, B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dB * learning_rate)\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss as step :{:03d}:{:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"W = {},B = {}\".format(W.numpy(), B.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfe.Variable 对象存储在训练期间访问的可变 tf.Tensor 值，以更加轻松地实现自动微分。模型的参数可以作为变量封装在类中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.036\nLoss at step 000: 66.346\nLoss at step 020: 30.158\nLoss at step 040: 14.004\nLoss at step 060: 6.792\nLoss at step 080: 3.573\nLoss at step 100: 2.135\nLoss at step 120: 1.493\nLoss at step 140: 1.207\nLoss at step 160: 1.079\nLoss at step 180: 1.021\nLoss at step 200: 0.996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 220: 0.984\nLoss at step 240: 0.979\nLoss at step 260: 0.977\nLoss at step 280: 0.976\nFinal loss: 0.976\nW = 3.0077202320098877, B = 2.0057666301727295\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# 输入 输出数据\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# 损失函数\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# 梯度函数\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "#迭代训练\n",
    "for i in range(300):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    # zip函数接受任意多个序列作为参数，将所有序列按相同的索引组合成一个元素是各个序列合并成的tuple的新序列，新的序列的长度以参数中最短的序列为准\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "计算梯度的其他函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n[<tf.Tensor: id=17922, shape=(), dtype=float32, numpy=6.0>]\n[<tf.Tensor: id=17934, shape=(), dtype=float32, numpy=2.0>]\n[None]\n[<tf.Tensor: id=159, shape=(), dtype=float32, numpy=1.0>]\n[<tf.Tensor: id=17959, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "print(square(3.))\n",
    "print(grad(3.))\n",
    "\n",
    "# 二次求导\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "print(gradgrad(3.))\n",
    "\n",
    "# 三次求导\n",
    "gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x)[0])\n",
    "print(gradgradgrad(3.))\n",
    "\n",
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "\n",
    "print(grad(3.))\n",
    "print(grad(-3.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=17970, shape=(), dtype=float32, numpy=0.5>]\n[<tf.Tensor: id=17981, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    # 自定义梯度的函数\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "print(grad_log1pexp(0.))\n",
    "\n",
    "print(grad_log1pexp(100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然 Eager Execution 增强了开发和调试的交互性，但 TensorFlow Graph Execution 在分布式训练、性能优化和生产部署方面具有优势。不过，编写图形代码不同于编写常规 Python 代码，并且更难以调试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
