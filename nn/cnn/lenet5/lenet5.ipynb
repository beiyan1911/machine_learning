{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nn.cnn.lenet5 import mnist_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28  # 图片尺寸\n",
    "num_labels = 10  #每张图片的输出标签个数\n",
    "num_channels = 1  #图片通道数（类似RGB）\n",
    "\n",
    "# leNet5\n",
    "batch_size = 20  #批次大小\n",
    "kernelSize = 5  #卷积模板尺寸\n",
    "depth1Size = 6  #第一卷积层深度\n",
    "depth2Size = 16  #第二卷积层深度\n",
    "padding = \"SAME\"  #填充方式\n",
    "convStride = 1  #卷积层移动单位\n",
    "poolStride = 2  #池化层移动单位\n",
    "poolFilterSize = 2  #池化层过滤器大小\n",
    "FC1HiddenUnit = 360  #全连接第一层隐藏单元\n",
    "FC2HiddenUnit = 256  #全连接第二层隐藏单元\n",
    "learningRate = 0.0001  #初始学习率\n",
    "LEARNING_RATE_DECAY = 0.99  #衰减率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n-------------------数据集信息-----------------------------\n\nTraining set   : (48000, 28, 28, 1) (48000, 10)\nValidation set : (6000, 28, 28, 1) (6000, 10)\nTest set       : (6000, 28, 28, 1) (6000, 10)\n\n---------------------------------------------------------\n\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "X_train, Y_train = mnist_utils.get_train_data()\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "test_data, valid_data, test_labels, valid_labels = train_test_split(test_data, test_labels, test_size=0.5,\n",
    "                                                                    random_state=0)\n",
    "del Y_train, X_train\n",
    "\n",
    "print('\\n-------------------数据集信息-----------------------------\\n')\n",
    "print('Training set   :', train_data.shape, train_labels.shape)\n",
    "print('Validation set :', valid_data.shape, valid_labels.shape)\n",
    "print('Test set       :', test_data.shape, test_labels.shape)\n",
    "print('\\n---------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size_pool(input_size, conv_filter_size, pool_filter_size, padding, conv_stride, pool_stride):\n",
    "    \"\"\"计算卷积池化之后图片的尺寸\"\"\"\n",
    "    if padding == 'SAME':\n",
    "        padding = 1.00\n",
    "    elif padding == 'VALID':\n",
    "        padding = 0.00\n",
    "    else:\n",
    "        return None\n",
    "    # 卷积后尺寸\n",
    "    output_1 = (((input_size - conv_filter_size + 2 * padding) / conv_stride) + 1.00)\n",
    "    # 池化后尺寸\n",
    "    output_2 = (((output_1 - pool_filter_size + 2 * padding) / pool_stride) + 1.00)\n",
    "    # 卷积后尺寸\n",
    "    output_3 = (((output_2 - conv_filter_size + 2 * padding) / conv_stride) + 1.00)\n",
    "    # 池化后尺寸\n",
    "    output_4 = (((output_3 - pool_filter_size + 2 * padding) / pool_stride) + 1.00)\n",
    "    return int(output_4)\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    \"\"\"计算正确率（百分比\"\"\"\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weightBuilder(shape, name):\n",
    "    \"\"\"定义变量\"\"\"\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name=name)\n",
    "\n",
    "\n",
    "def biasesBuilder(shape, name):\n",
    "    \"\"\"定义bias项\"\"\"\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape), name=name)\n",
    "\n",
    "\n",
    "def conv2d(x, W, name):\n",
    "    \"\"\"定义二维卷积\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding, name=name)\n",
    "\n",
    "\n",
    "def maxPool_2x2(x, name):\n",
    "    \"\"\"定义池化\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义lenet5网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 卷积池化之后的图像尺寸\n",
    "finalImageSize = output_size_pool(input_size=image_size, conv_filter_size=kernelSize,\n",
    "                                  pool_filter_size=poolFilterSize, padding=padding,\n",
    "                                  conv_stride=convStride, pool_stride=poolStride)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    # 验证数据集 定义为常量\n",
    "    tf_valid_dataset = tf.constant(valid_data)\n",
    "    # 测试数据集 定义为常量\n",
    "    tf_test_dataset = tf.constant(test_data)\n",
    "\n",
    "    with tf.name_scope('convolution1') as scope:\n",
    "        # weight & biases\n",
    "        C1_w = weightBuilder([kernelSize, kernelSize, num_channels, depth1Size], \"C1_w\")\n",
    "        C1_b = biasesBuilder([depth1Size], \"C1_b\")\n",
    "\n",
    "    with tf.name_scope('convolution2') as scope:\n",
    "        C2_w = weightBuilder([kernelSize, kernelSize, depth1Size, depth2Size], \"C2_w\")\n",
    "        C2_b = biasesBuilder([depth2Size], \"C2_b\")\n",
    "\n",
    "    with tf.name_scope('fullyConct1') as scope:\n",
    "        FC1_w = weightBuilder([finalImageSize * finalImageSize * depth2Size, FC1HiddenUnit], \"FC1_w\")\n",
    "        FC1_b = biasesBuilder([FC1HiddenUnit], \"FC1_b\")\n",
    "        keep_prob = tf.placeholder(dtype=tf.float32, name=\"keepProb\")\n",
    "\n",
    "    with tf.name_scope('fullyConct2') as scope:\n",
    "        FC2_w = weightBuilder([FC1HiddenUnit, FC2HiddenUnit], \"FC2_w\")\n",
    "        FC2_b = biasesBuilder([FC2HiddenUnit], \"FC2_b\")\n",
    "\n",
    "    with tf.name_scope('fullyConct3') as scope:\n",
    "        FC3_w = weightBuilder([FC2HiddenUnit, num_labels], \"FC3_w\")\n",
    "        FC3_b = biasesBuilder([num_labels], \"FC3_b\")\n",
    "\n",
    "        #定义lenet5 前向传播\n",
    "    def leNet5(data):\n",
    "        # C1\n",
    "        h_conv = tf.nn.relu(conv2d(data, C1_w, \"conv1\") + C1_b)\n",
    "        # P2\n",
    "        h_pool = maxPool_2x2(h_conv, \"pool1\")\n",
    "\n",
    "        # C3\n",
    "        h_conv = tf.nn.relu(conv2d(h_pool, C2_w, \"conv2\") + C2_b)\n",
    "        # P4\n",
    "        h_pool = maxPool_2x2(h_conv, \"pool2\")\n",
    "\n",
    "        # reshape last conv layer\n",
    "        shape = h_pool.get_shape().as_list()\n",
    "        h_pool_reshaped = tf.reshape(h_pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        # F5\n",
    "        h_FC1 = tf.nn.relu(tf.matmul(h_pool_reshaped, FC1_w) + FC1_b)\n",
    "        h_FC1 = tf.nn.dropout(h_FC1, keep_prob=keep_prob)\n",
    "\n",
    "        # F6\n",
    "        h_FC2 = tf.nn.relu(tf.matmul(h_FC1, FC2_w) + FC2_b)\n",
    "\n",
    "        # OUTPUT\n",
    "        model = tf.matmul(h_FC2, FC3_w) + FC3_b\n",
    "        return model\n",
    "\n",
    "\n",
    "    # train computation\n",
    "    logits = leNet5(tf_train_dataset)\n",
    "    # 损失函数\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))\n",
    "\n",
    "    # 指数衰减学习率\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learningRate, global_step, train_data.shape[0] / batch_size,\n",
    "                                               LEARNING_RATE_DECAY, staircase=True)\n",
    "    # 优化\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n",
    "    # 训练，验证，测试集的预测结果\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(leNet5(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(leNet5(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练并输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  |loss           |MiniBatch acc  |valid acc      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     |2.335149765    |10.000000000   |10.083333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200   |2.308820248    |5.000000000    |10.583333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400   |2.318854332    |10.000000000   |9.883333333    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600   |2.220359087    |15.000000000   |20.600000000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800   |1.764576316    |30.000000000   |31.083333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  |1.371842146    |45.000000000   |57.333333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200  |0.624609172    |75.000000000   |73.316666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400  |1.403442025    |50.000000000   |79.166666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600  |0.741630673    |75.000000000   |82.433333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800  |0.515490890    |85.000000000   |82.716666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  |0.289221197    |90.000000000   |86.733333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200  |0.529879570    |80.000000000   |87.833333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch: 0001 cost= 1.271519833 \n\nStep  |loss           |MiniBatch acc  |valid acc      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     |0.691753268    |80.000000000   |88.900000000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200   |0.128713295    |100.000000000  |89.816666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400   |0.067379273    |100.000000000  |89.500000000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600   |0.143912867    |95.000000000   |90.416666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800   |0.341083825    |90.000000000   |91.400000000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  |0.174044400    |90.000000000   |91.766666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200  |0.106899783    |100.000000000  |92.483333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400  |0.690200210    |75.000000000   |92.633333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600  |0.109394670    |100.000000000  |92.766666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800  |0.148851067    |90.000000000   |93.216666667   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000  |0.128434226    |95.000000000   |93.433333333   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200  |0.293642402    |85.000000000   |93.750000000   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch: 0002 cost= 0.315141845 \n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 数据集正确率: 94.316667%\n"
     ]
    }
   ],
   "source": [
    "# 循环轮数\n",
    "num_epochs = 2\n",
    "# 每一轮批次数\n",
    "total_batch = int(train_data.shape[0] / batch_size)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0.\n",
    "        print('{:6}|{:15}|{:15}|{:15}'.format(\"Step\", \"loss\", \"MiniBatch acc\", \"valid acc\"))\n",
    "        for step in range(total_batch):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, keep_prob: 0.5}\n",
    "            # train\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            # 计算平均损失值\n",
    "            avg_cost += l / total_batch\n",
    "            # 200批次进行输出\n",
    "            if (step % 200 == 0):\n",
    "                valid_pred = session.run(valid_prediction, feed_dict={keep_prob: 1})\n",
    "                print('{:6}|{:15}|{:15}|{:15}'.format(\"{:d}\".format(step),\n",
    "                                                      \"{:.9f}\".format(l),\n",
    "                                                      \"{:.9f}\".format(accuracy(predictions, batch_labels)),\n",
    "                                                      \"{:.9f}\".format(accuracy(valid_pred, valid_labels))))\n",
    "        # end for batch\n",
    "\n",
    "        print(\"\\nEpoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost), \"\\n\")\n",
    "\n",
    "    test_pred = session.run(test_prediction, feed_dict={keep_prob: 1})\n",
    "    print(\"Test 数据集正确率: %.6f%%\" % accuracy(test_pred, test_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
